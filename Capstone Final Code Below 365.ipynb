{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to keep only rows where 'treatment_pd' is lesser than or equal to 365\n",
    "df = df[df['treatment_pd'] <= 365]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null values in the 'payer_type' column with 'UNINSURED'\n",
    "df['payer_type'] = df['payer_type'].fillna('UNINSURED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_count = df['patient_state'].nunique()\n",
    "unique_values = df['patient_state'].unique()\n",
    "\n",
    "print(\"Number of unique values in the column:\", unique_count)\n",
    "print(\"Unique values in the column:\\n\", unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('ZIP3 Region Division.csv')\n",
    "\n",
    "# Merge the datasets on 'patient_zip3'\n",
    "merged_df = pd.merge(df, df1, how='left', left_on='patient_zip3', right_on='Zip Code')\n",
    "\n",
    "# Replace incorrect values in 'patient_state' with the corresponding values from 'State Code'\n",
    "merged_df['patient_state'] = merged_df['State Code'].combine_first(merged_df['patient_state'])\n",
    "merged_df['region'] = merged_df['State Region'].combine_first(merged_df['region'])\n",
    "merged_df['division'] = merged_df['State Division'].combine_first(merged_df['division'])\n",
    "\n",
    "# Drop unnecessary columns if needed\n",
    "df = merged_df.drop(['Zip Code', 'State Code', 'State Name', 'State Region', 'State Division'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_count = df['patient_state'].nunique()\n",
    "unique_values = df['patient_state'].unique()\n",
    "\n",
    "print(\"Number of unique values in the column:\", unique_count)\n",
    "print(\"Unique values in the column:\\n\", unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null values in the 'region' and'division' column with 'Others'\n",
    "df['region'] = df['region'].fillna('Others')\n",
    "df['division'] = df['division'].fillna('Others')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()[df.isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing ICD9 codes with ICD10 codes\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"174\", \"C50019\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"1741\", \"C50119\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"1742\", \"C50219\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"1743\", \"C50319\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"1744\", \"C50419\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"1745\", \"C50519\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"1746\", \"C50619\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"1748\", \"C50819\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"1749\", \"C50919\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"1759\", \"C50919\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"19881\", \"C50919\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting the ICD10 codes \n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"C50\", \"C50919\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"C5001\", \"C50019\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"C50021\", \"C50011\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"C5011\", \"C50119\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"C50121\", \"C50111\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"C5021\", \"C50219\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"C50221\", \"C50211\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"C50222\", \"C50212\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"C5031\", \"C50319\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"C50322\", \"C50312\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"C5041\", \"C50419\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"C50421\", \"C50411\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"C50422\", \"C50412\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"C5051\", \"C50519\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"C50521\", \"C50511\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"C50522\", \"C50512\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"C5061\", \"C50619\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"C5081\", \"C50819\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"C5091\", \"C50919\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"C50921\", \"C50911\")\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].replace(\"C50929\", \"C50919\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the breast cancer diagnosis description according to the breast cancer diagnosis code\n",
    "df.loc[df['breast_cancer_diagnosis_code'] == 'C50011', 'breast_cancer_diagnosis_desc'] = 'Malignant neoplasm of nipple and areola of right female breast'\n",
    "df.loc[df['breast_cancer_diagnosis_code'] == 'C50012', 'breast_cancer_diagnosis_desc'] = 'Malignant neoplasm of nipple and areola of left female breast'\n",
    "df.loc[df['breast_cancer_diagnosis_code'] == 'C50019', 'breast_cancer_diagnosis_desc'] = 'Malignant neoplasm of nipple and areola of unspecified female breast'\n",
    "\n",
    "df.loc[df['breast_cancer_diagnosis_code'] == 'C50111', 'breast_cancer_diagnosis_desc'] = 'Malignant neoplasm of central portion of right female breast'\n",
    "df.loc[df['breast_cancer_diagnosis_code'] == 'C50112', 'breast_cancer_diagnosis_desc'] = 'Malignant neoplasm of central portion of left female breast'\n",
    "df.loc[df['breast_cancer_diagnosis_code'] == 'C50119', 'breast_cancer_diagnosis_desc'] = 'Malignant neoplasm of central portion of unspecified female breast'\n",
    "\n",
    "df.loc[df['breast_cancer_diagnosis_code'] == 'C50211', 'breast_cancer_diagnosis_desc'] = 'Malignant neoplasm of upper-inner quadrant of right female breast'\n",
    "df.loc[df['breast_cancer_diagnosis_code'] == 'C50212', 'breast_cancer_diagnosis_desc'] = 'Malignant neoplasm of upper-inner quadrant of left female breast'\n",
    "df.loc[df['breast_cancer_diagnosis_code'] == 'C50219', 'breast_cancer_diagnosis_desc'] = 'Malignant neoplasm of upper-inner quadrant of unspecified female breast'\n",
    "\n",
    "df.loc[df['breast_cancer_diagnosis_code'] == 'C50311', 'breast_cancer_diagnosis_desc'] = 'Malignant neoplasm of lower-inner quadrant of right female breast'\n",
    "df.loc[df['breast_cancer_diagnosis_code'] == 'C50312', 'breast_cancer_diagnosis_desc'] = 'Malignant neoplasm of lower-inner quadrant of left female breast'\n",
    "df.loc[df['breast_cancer_diagnosis_code'] == 'C50319', 'breast_cancer_diagnosis_desc'] = 'Malignant neoplasm of lower-inner quadrant of unspecified female breast'\n",
    "\n",
    "df.loc[df['breast_cancer_diagnosis_code'] == 'C50411', 'breast_cancer_diagnosis_desc'] = 'Malignant neoplasm of upper-outer quadrant of right female breast'\n",
    "df.loc[df['breast_cancer_diagnosis_code'] == 'C50412', 'breast_cancer_diagnosis_desc'] = 'Malignant neoplasm of upper-outer quadrant of left female breast'\n",
    "df.loc[df['breast_cancer_diagnosis_code'] == 'C50419', 'breast_cancer_diagnosis_desc'] = 'Malignant neoplasm of upper-outer quadrant of unspecified female breast'\n",
    "\n",
    "df.loc[df['breast_cancer_diagnosis_code'] == 'C50511', 'breast_cancer_diagnosis_desc'] = 'Malignant neoplasm of lower-outer quadrant of right female breast'\n",
    "df.loc[df['breast_cancer_diagnosis_code'] == 'C50512', 'breast_cancer_diagnosis_desc'] = 'Malignant neoplasm of lower-outer quadrant of left female breast'\n",
    "df.loc[df['breast_cancer_diagnosis_code'] == 'C50519', 'breast_cancer_diagnosis_desc'] = 'Malignant neoplasm of lower-outer quadrant of unspecified female breast'\n",
    "\n",
    "df.loc[df['breast_cancer_diagnosis_code'] == 'C50611', 'breast_cancer_diagnosis_desc'] = 'Malignant neoplasm of axillary tail of right female breast'\n",
    "df.loc[df['breast_cancer_diagnosis_code'] == 'C50612', 'breast_cancer_diagnosis_desc'] = 'Malignant neoplasm of axillary tail of left female breast'\n",
    "df.loc[df['breast_cancer_diagnosis_code'] == 'C50619', 'breast_cancer_diagnosis_desc'] = 'Malignant neoplasm of axillary tail of unspecified female breast'\n",
    "\n",
    "df.loc[df['breast_cancer_diagnosis_code'] == 'C50811', 'breast_cancer_diagnosis_desc'] = 'Malignant neoplasm of overlapping sites of right female breast'\n",
    "df.loc[df['breast_cancer_diagnosis_code'] == 'C50812', 'breast_cancer_diagnosis_desc'] = 'Malignant neoplasm of overlapping sites of left female breast'\n",
    "df.loc[df['breast_cancer_diagnosis_code'] == 'C50819', 'breast_cancer_diagnosis_desc'] = 'Malignant neoplasm of overlapping sites of unspecified female breast'\n",
    "\n",
    "df.loc[df['breast_cancer_diagnosis_code'] == 'C50911', 'breast_cancer_diagnosis_desc'] = 'Malignant neoplasm of unspecified site of right female breast'\n",
    "df.loc[df['breast_cancer_diagnosis_code'] == 'C50912', 'breast_cancer_diagnosis_desc'] = 'Malignant neoplasm of unspecified site of left female breast'\n",
    "df.loc[df['breast_cancer_diagnosis_code'] == 'C50919', 'breast_cancer_diagnosis_desc'] = 'Malignant neoplasm of unspecified site of unspecified female breast'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_count = df['metastatic_first_treatment'].nunique()\n",
    "unique_values = df['metastatic_first_treatment'].unique()\n",
    "\n",
    "print(\"Number of unique values in the column:\", unique_count)\n",
    "print(\"Unique values in the column:\\n\", unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['metastatic_first_treatment'] = df['metastatic_first_treatment'].replace('DOCETAXEL ANHYDROUS', 'DOCETAXEL')\n",
    "df['metastatic_first_treatment'] = df['metastatic_first_treatment'].replace('DOXORUBICIN HCL', 'DOXORUBICIN HYDROCHLORIDE')\n",
    "df['metastatic_first_treatment'] = df['metastatic_first_treatment'].replace('DOXORUBICIN HCL LIPOSOMAL', 'DOXORUBICIN HYDROCHLORIDE LIPOSOME')\n",
    "df['metastatic_first_treatment'] = df['metastatic_first_treatment'].replace('DRUG ASSAY EVEROLIMUS', 'EVEROLIMUS')\n",
    "df['metastatic_first_treatment'] = df['metastatic_first_treatment'].replace('EPIRUBICIN HCL', 'EPIRUBICIN HYDROCHLORIDE')\n",
    "df['metastatic_first_treatment'] = df['metastatic_first_treatment'].replace('GEMCITABINE HCL', 'GEMCITABINE HYDROCHLORIDE')\n",
    "df['metastatic_first_treatment'] = df['metastatic_first_treatment'].replace('Inj gemcitabine hcl (accord)', 'GEMCITABINE HYDROCHLORIDE')\n",
    "df['metastatic_first_treatment'] = df['metastatic_first_treatment'].replace('IRINOTECAN HCL', 'IRINOTECAN HYDROCHLORIDE')\n",
    "df['metastatic_first_treatment'] = df['metastatic_first_treatment'].replace('METHOTREXATE', 'METHOTREXATE SODIUM')\n",
    "df['metastatic_first_treatment'] = df['metastatic_first_treatment'].replace('NIRAPARIB', 'NIRAPARIB TOSYLATE MONOHYDRATE')\n",
    "df['metastatic_first_treatment'] = df['metastatic_first_treatment'].replace('PACLITAXEL PROTEIN BOUND PARTICLES', 'PACLITAXEL NANOPARTICLE')\n",
    "df['metastatic_first_treatment'] = df['metastatic_first_treatment'].replace('PEMETREXED DISODIUM HEPTAHYDRATE', 'PEMETREXED DISODIUM')\n",
    "df['metastatic_first_treatment'] = df['metastatic_first_treatment'].replace('TOPOTECAN HCL', 'TOPOTECAN HYDROCHLORIDE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_count = df['metastatic_first_treatment'].nunique()\n",
    "unique_values = df['metastatic_first_treatment'].unique()\n",
    "\n",
    "print(\"Number of unique values in the column:\", unique_count)\n",
    "print(\"Unique values in the column:\\n\", unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping drugs to treatment types with capitalized names\n",
    "treatment_mapping = {\n",
    "    'ATEZOLIZUMAB': 'Immunotherapy',\n",
    "    'BEVACIZUMAB': 'Targeted Therapy',\n",
    "    'BLEOMYCIN SULFATE': 'Chemotherapy',\n",
    "    'CAPECITABINE': 'Chemotherapy',\n",
    "    'CARBOPLATIN': 'Chemotherapy',\n",
    "    'CISPLATIN': 'Chemotherapy',\n",
    "    'CYCLOPHOSPHAMIDE': 'Chemotherapy',\n",
    "    'DOCETAXEL': 'Chemotherapy',\n",
    "    'DOXORUBICIN HYDROCHLORIDE': 'Chemotherapy',\n",
    "    'DOXORUBICIN HYDROCHLORIDE LIPOSOME': 'Chemotherapy',\n",
    "    'EVEROLIMUS': 'Targeted Therapy',\n",
    "    'EPIRUBICIN HYDROCHLORIDE': 'Chemotherapy',\n",
    "    'ERIBULIN MESYLATE': 'Chemotherapy',\n",
    "    'ERLOTINIB HYDROCHLORIDE': 'Targeted Therapy',\n",
    "    'ETOPOSIDE': 'Chemotherapy',\n",
    "    'GEMCITABINE HYDROCHLORIDE': 'Chemotherapy',\n",
    "    'IPILIMUMAB': 'Immunotherapy',\n",
    "    'IRINOTECAN HYDROCHLORIDE': 'Chemotherapy',\n",
    "    'IXABEPILONE': 'Chemotherapy',\n",
    "    'METHOTREXATE SODIUM': 'Chemotherapy',\n",
    "    'NIRAPARIB TOSYLATE MONOHYDRATE': 'Targeted Therapy',\n",
    "    'NIVOLUMAB': 'Immunotherapy',\n",
    "    'PACLITAXEL': 'Chemotherapy',\n",
    "    'PACLITAXEL NANOPARTICLE': 'Chemotherapy',\n",
    "    'PAZOPANIB HYDROCHLORIDE': 'Targeted Therapy',\n",
    "    'PEMETREXED DISODIUM': 'Chemotherapy',\n",
    "    'RUCAPARIB CAMSYLATE': 'Targeted Therapy',\n",
    "    'TALAZOPARIB TOSYLATE': 'Targeted Therapy',\n",
    "    'TEMOZOLOMIDE': 'Chemotherapy',\n",
    "    'THALIDOMIDE': 'Targeted Therapy',\n",
    "    'TOPOTECAN HYDROCHLORIDE': 'Chemotherapy',\n",
    "    'VINORELBINE TARTRATE': 'Chemotherapy'\n",
    "}\n",
    "\n",
    "# Map the treatment types based on the metastatic_first_treatment column\n",
    "df['metastatic_first_treatment_type'] = df['metastatic_first_treatment'].map(treatment_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()[df.isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with a high number of missing values\n",
    "df = df.drop(['patient_race', 'bmi', 'metastatic_first_novel_treatment', 'metastatic_first_novel_treatment_type'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_null_values = df.isnull().sum()[df.isnull().sum() > 0]\n",
    "print(\"Columns with null values and their respective counts:\")\n",
    "print(columns_with_null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness = df['self_employed'].skew()\n",
    "print(\"Skewness of the column:\", skewness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null values in 'self_employed' column with the median\n",
    "df['self_employed'].fillna(df['self_employed'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness = df['farmer'].skew()\n",
    "print(\"Skewness of the column:\", skewness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null values in 'farmer' column with the median\n",
    "df['farmer'].fillna(df['farmer'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_null_values = df.isnull().sum()[df.isnull().sum() > 0]\n",
    "print(\"Columns with null values and their respective counts:\")\n",
    "print(columns_with_null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with null values in specific columns\n",
    "df.dropna(subset=['family_size', 'family_dual_income', 'income_household_median','home_ownership', 'home_value', 'rent_median','rent_burden', 'poverty', 'limited_english'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of 'treatment_pd' for each 'patient_zip3' code\n",
    "average_treatment_by_zip3 = df.groupby('patient_zip3')['treatment_pd'].mean().reset_index()\n",
    "\n",
    "# Merge the mean values back to the original DataFrame\n",
    "df = pd.merge(df, average_treatment_by_zip3, how='left', on='patient_zip3')\n",
    "\n",
    "# Create a new column called 'Mean_treatment_pd'\n",
    "df['Mean_treatment_pd'] = df['treatment_pd_y']\n",
    "\n",
    "# Drop unnecessary columns if needed\n",
    "df = df.drop(['treatment_pd_y'], axis=1)\n",
    "\n",
    "# Rename 'treatment_pd_x' to 'treatment_pd'\n",
    "df = df.rename(columns={'treatment_pd_x': 'treatment_pd'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the median of 'treatment_pd' for each 'patient_zip3' code\n",
    "median_treatment_by_zip3 = df.groupby('patient_zip3')['treatment_pd'].median().reset_index()\n",
    "\n",
    "# Merge the median values back to the original DataFrame\n",
    "df = pd.merge(df, median_treatment_by_zip3, how='left', on='patient_zip3')\n",
    "\n",
    "# Create a new column called 'Median_treatment_pd'\n",
    "df['Median_treatment_pd'] = df['treatment_pd_y']\n",
    "\n",
    "# Drop unnecessary columns if needed\n",
    "df = df.drop(['treatment_pd_y'], axis=1)\n",
    "\n",
    "# Rename 'treatment_pd_x' to 'treatment_pd'\n",
    "df = df.rename(columns={'treatment_pd_x': 'treatment_pd'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical columns to the appropriate data types\n",
    "df['patient_gender'] = df['patient_gender'].astype('category')\n",
    "df['breast_cancer_diagnosis_code'] = df['breast_cancer_diagnosis_code'].astype('category')\n",
    "df['breast_cancer_diagnosis_desc'] = df['breast_cancer_diagnosis_desc'].astype('category')\n",
    "df['metastatic_cancer_diagnosis_code'] = df['metastatic_cancer_diagnosis_code'].astype('category')\n",
    "df['metastatic_first_treatment'] = df['metastatic_first_treatment'].astype('category')\n",
    "df['region'] = df['region'].astype('category')\n",
    "df['division'] = df['division'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Verify the data types after cleaning\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(150, 150))\n",
    "\n",
    "# Generate a heatmap with values annotated\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=.5)\n",
    "\n",
    "# Set the title and show the plot\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set(style=\"ticks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bins for treatment_pd\n",
    "bins = [-1, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500]\n",
    "\n",
    "# Create a new column 'treatment_pd_bins' with the bin labels\n",
    "df['treatment_pd_bins'] = pd.cut(df['treatment_pd'], bins=bins)\n",
    "\n",
    "# Plotting the count of treatment_pd with binning\n",
    "plt.figure(figsize=(15, 10))\n",
    "ax = sns.countplot(y='treatment_pd_bins', data=df, palette='autumn')\n",
    "\n",
    "# Annotating each bar with its count\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_width(), '.0f'), \n",
    "                 (p.get_width(), p.get_y() + p.get_height() / 2), \n",
    "                 ha='left', \n",
    "                 va='center', \n",
    "                 xytext=(5, 0), \n",
    "                 textcoords='offset points',\n",
    "                 fontsize=10)\n",
    "\n",
    "plt.title('Distribution of time period between Breast Cancer diagnosis and treatment')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Duration between diagnosis and treatment')\n",
    "\n",
    "# Changing y-axis labels\n",
    "new_labels = ['0-100', '101-200', '201-300', '301-400', '401-500', '501-600', '601-700', '701-800', '801-900', '901-1000', '1001-1100', '1101-1200', '1201-1300', '1301-1400', '1401-1500']\n",
    "ax.set_yticklabels(new_labels)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count plot for Payer Type\n",
    "plt.figure(figsize=(15, 5))\n",
    "ax = sns.countplot(y='payer_type', data=df, palette='autumn', order=sorted(df['payer_type'].unique()))\n",
    "\n",
    "# Annotate each bar with its count value\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_width(), '.0f'), \n",
    "                 (p.get_width(), p.get_y() + p.get_height() / 2), \n",
    "                 ha = 'left', \n",
    "                 va = 'center', \n",
    "                 xytext = (5, 0), \n",
    "                 textcoords = 'offset points')\n",
    "\n",
    "plt.title('Count of Payer Types')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Payer Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean treatment_pd for each payer_type\n",
    "mean_treatment_pd = df.groupby('payer_type')['treatment_pd'].mean().reset_index()\n",
    "\n",
    "# Bar plot for Average Treatment PD by Payer Type\n",
    "plt.figure(figsize=(15, 5))\n",
    "ax = sns.barplot(x='treatment_pd', y='payer_type', data=mean_treatment_pd, palette='autumn')\n",
    "\n",
    "# Annotate each bar with its mean treatment PD value\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_width(), '.2f'), \n",
    "                 (p.get_width(), p.get_y() + p.get_height() / 2), \n",
    "                 ha = 'left', \n",
    "                 va = 'center', \n",
    "                 xytext = (5, 0), \n",
    "                 textcoords = 'offset points')\n",
    "\n",
    "plt.title('Mean Treatment PD by Payer Type')\n",
    "plt.xlabel('Mean Treatment PD')\n",
    "plt.ylabel('Payer Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a count plot with switched axes and arranged patient states alphabetically\n",
    "plt.figure(figsize=(15, 15))\n",
    "ax = sns.countplot(y='patient_state', data=df, palette='autumn', order=sorted(df['patient_state'].unique()))\n",
    "\n",
    "# Annotate each bar with its count value\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_width(), '.0f'), \n",
    "                 (p.get_width(), p.get_y() + p.get_height() / 2), \n",
    "                 ha='left', \n",
    "                 va='center', \n",
    "                 xytext=(5, 0), \n",
    "                 textcoords='offset points')\n",
    "\n",
    "plt.title('Count of Patient States')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Patient State')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean treatment_pd for each patient_state\n",
    "mean_treatment_pd_state = df.groupby('patient_state')['treatment_pd'].mean().reset_index()\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(15, 15))\n",
    "ax = sns.barplot(x='treatment_pd', y='patient_state', data=mean_treatment_pd_state, palette='autumn', orient='h')\n",
    "\n",
    "# Annotate each bar with its mean treatment PD value\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_width(), '.2f'), \n",
    "                 (p.get_width(), p.get_y() + p.get_height() / 2), \n",
    "                 ha='left', \n",
    "                 va='center', \n",
    "                 xytext=(5, 0), \n",
    "                 textcoords='offset points')\n",
    "\n",
    "plt.title('Mean Treatment PD by Patient State')\n",
    "plt.xlabel('Mean Treatment PD')\n",
    "plt.ylabel('Patient State')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "ax = sns.countplot(y='patient_age', data=df, palette='autumn', order=sorted(df['patient_age'].unique()))\n",
    "\n",
    "# Annotate each bar with its count value\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_width(), '.0f'), \n",
    "                 (p.get_width(), p.get_y() + p.get_height() / 2), \n",
    "                 ha = 'left', \n",
    "                 va = 'center', \n",
    "                 xytext = (5, 0), \n",
    "                 textcoords = 'offset points')\n",
    "\n",
    "plt.title('Count of Patient Ages')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Patient Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean treatment_pd for each patient_age\n",
    "mean_treatment_pd_age = df.groupby('patient_age')['treatment_pd'].mean().reset_index()\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(15, 15))\n",
    "ax = sns.barplot(x='treatment_pd', y='patient_age', data=mean_treatment_pd_age, palette='autumn', orient='h')\n",
    "\n",
    "# Annotate each bar with its mean treatment PD value\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_width(), '.2f'), \n",
    "                 (p.get_width(), p.get_y() + p.get_height() / 2), \n",
    "                 ha='left', \n",
    "                 va='center', \n",
    "                 xytext=(5, 0), \n",
    "                 textcoords='offset points')\n",
    "\n",
    "plt.title('Mean Treatment PD by Patient Age')\n",
    "plt.xlabel('Mean Treatment PD')\n",
    "plt.ylabel('Patient Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a count plot with switched axes and arranged breast cancer diagnosis codes alphabetically\n",
    "plt.figure(figsize=(15, 10))\n",
    "ax = sns.countplot(y='breast_cancer_diagnosis_code', data=df, palette='autumn', order=sorted(df['breast_cancer_diagnosis_code'].unique()))\n",
    "\n",
    "# Annotate each bar with its count value\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_width(), '.0f'), \n",
    "                 (p.get_width(), p.get_y() + p.get_height() / 2), \n",
    "                 ha='left', \n",
    "                 va='center', \n",
    "                 xytext=(5, 0), \n",
    "                 textcoords='offset points')\n",
    "\n",
    "plt.title('Count of Breast Cancer Diagnosis Codes')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Breast Cancer Diagnosis Code')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean treatment_pd for each breast_cancer_diagnosis_code\n",
    "mean_treatment_pd_code = df.groupby('breast_cancer_diagnosis_code')['treatment_pd'].mean().reset_index()\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "ax = sns.barplot(x='treatment_pd', y='breast_cancer_diagnosis_code', data=mean_treatment_pd_code, palette='autumn', orient='h')\n",
    "\n",
    "# Annotate each bar with its mean treatment PD value\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_width(), '.2f'), \n",
    "                 (p.get_width(), p.get_y() + p.get_height() / 2), \n",
    "                 ha='left', \n",
    "                 va='center', \n",
    "                 xytext=(5, 0), \n",
    "                 textcoords='offset points')\n",
    "\n",
    "plt.title('Mean Treatment PD by Breast Cancer Diagnosis Code')\n",
    "plt.xlabel('Mean Treatment PD')\n",
    "plt.ylabel('Breast Cancer Diagnosis Code')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a count plot with switched axes and arranged breast cancer diagnosis years alphabetically\n",
    "plt.figure(figsize=(15, 5))\n",
    "ax = sns.countplot(y='breast_cancer_diagnosis_year', data=df, palette='autumn', order=sorted(df['breast_cancer_diagnosis_year'].unique()))\n",
    "\n",
    "# Annotate each bar with its count value\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_width(), '.0f'), \n",
    "                 (p.get_width(), p.get_y() + p.get_height() / 2), \n",
    "                 ha='left', \n",
    "                 va='center', \n",
    "                 xytext=(5, 0), \n",
    "                 textcoords='offset points')\n",
    "\n",
    "plt.title('Count of Breast Cancer Diagnosis Years')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Breast Cancer Diagnosis Year')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean treatment_pd for each breast_cancer_diagnosis_year\n",
    "mean_treatment_pd_year = df.groupby('breast_cancer_diagnosis_year')['treatment_pd'].mean().reset_index()\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "ax = sns.barplot(x='treatment_pd', y='breast_cancer_diagnosis_year', data=mean_treatment_pd_year, palette='autumn', orient='h')\n",
    "\n",
    "# Annotate each bar with its mean treatment PD value\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_width(), '.2f'), \n",
    "                 (p.get_width(), p.get_y() + p.get_height() / 2), \n",
    "                 ha='left', \n",
    "                 va='center', \n",
    "                 xytext=(5, 0), \n",
    "                 textcoords='offset points')\n",
    "\n",
    "plt.title('Mean Treatment PD by Breast Cancer Diagnosis Year')\n",
    "plt.xlabel('Mean Treatment PD')\n",
    "plt.ylabel('Breast Cancer Diagnosis Year')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a count plot with switched axes and arranged metastatic cancer diagnosis codes alphabetically\n",
    "plt.figure(figsize=(15, 10))\n",
    "ax = sns.countplot(y='metastatic_cancer_diagnosis_code', data=df, palette='autumn', order=sorted(df['metastatic_cancer_diagnosis_code'].unique()))\n",
    "\n",
    "# Annotate each bar with its count value\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_width(), '.0f'), \n",
    "                 (p.get_width(), p.get_y() + p.get_height() / 2), \n",
    "                 ha='left', \n",
    "                 va='center', \n",
    "                 xytext=(5, 0), \n",
    "                 textcoords='offset points')\n",
    "\n",
    "plt.title('Count of Metastatic Cancer Diagnosis Codes')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Metastatic Cancer Diagnosis Code')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean treatment_pd for each metastatic_cancer_diagnosis_code\n",
    "mean_treatment_pd_metastatic = df.groupby('metastatic_cancer_diagnosis_code')['treatment_pd'].mean().reset_index()\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "ax = sns.barplot(x='treatment_pd', y='metastatic_cancer_diagnosis_code', data=mean_treatment_pd_metastatic, palette='autumn', orient='h')\n",
    "\n",
    "# Annotate each bar with its mean treatment PD value\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_width(), '.2f'), \n",
    "                 (p.get_width(), p.get_y() + p.get_height() / 2), \n",
    "                 ha='left', \n",
    "                 va='center', \n",
    "                 xytext=(5, 0), \n",
    "                 textcoords='offset points')\n",
    "\n",
    "plt.title('Mean Treatment PD by Metastatic Cancer Diagnosis Code')\n",
    "plt.xlabel('Mean Treatment PD')\n",
    "plt.ylabel('Metastatic Cancer Diagnosis Code')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a count plot with switched axes and arranged metastatic first treatment codes alphabetically\n",
    "plt.figure(figsize=(15, 10))\n",
    "ax = sns.countplot(y='metastatic_first_treatment', data=df, palette='autumn', order=sorted(df['metastatic_first_treatment'].unique()))\n",
    "\n",
    "# Annotate each bar with its count value\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_width(), '.0f'), \n",
    "                 (p.get_width(), p.get_y() + p.get_height() / 2), \n",
    "                 ha='left', \n",
    "                 va='center', \n",
    "                 xytext=(5, 0), \n",
    "                 textcoords='offset points')\n",
    "\n",
    "plt.title('Count of Metastatic First Treatment Codes')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Metastatic First Treatment Code')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean treatment_pd for each metastatic_first_treatment\n",
    "mean_treatment_pd_first_treatment = df.groupby('metastatic_first_treatment')['treatment_pd'].mean().reset_index()\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "ax = sns.barplot(x='treatment_pd', y='metastatic_first_treatment', data=mean_treatment_pd_first_treatment, palette='autumn', orient='h')\n",
    "\n",
    "# Annotate each bar with its mean treatment PD value\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_width(), '.2f'), \n",
    "                 (p.get_width(), p.get_y() + p.get_height() / 2), \n",
    "                 ha='left', \n",
    "                 va='center', \n",
    "                 xytext=(5, 0), \n",
    "                 textcoords='offset points')\n",
    "\n",
    "plt.title('Mean Treatment PD by Metastatic First Treatment')\n",
    "plt.xlabel('Mean Treatment PD')\n",
    "plt.ylabel('Metastatic First Treatment')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a count plot with switched axes and arranged metastatic first treatment types alphabetically\n",
    "plt.figure(figsize=(15, 5))\n",
    "ax = sns.countplot(y='metastatic_first_treatment_type', data=df, palette='autumn', order=sorted(df['metastatic_first_treatment_type'].unique()))\n",
    "\n",
    "# Annotate each bar with its count value\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_width(), '.0f'), \n",
    "                 (p.get_width(), p.get_y() + p.get_height() / 2), \n",
    "                 ha='left', \n",
    "                 va='center', \n",
    "                 xytext=(5, 0), \n",
    "                 textcoords='offset points')\n",
    "\n",
    "plt.title('Count of Metastatic First Treatment Types')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Metastatic First Treatment Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean treatment_pd for each metastatic_first_treatment_type\n",
    "mean_treatment_pd_first_treatment_type = df.groupby('metastatic_first_treatment_type')['treatment_pd'].mean().reset_index()\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "ax = sns.barplot(x='treatment_pd', y='metastatic_first_treatment_type', data=mean_treatment_pd_first_treatment_type, palette='autumn', orient='h')\n",
    "\n",
    "# Annotate each bar with its mean treatment PD value\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_width(), '.2f'), \n",
    "                 (p.get_width(), p.get_y() + p.get_height() / 2), \n",
    "                 ha='left', \n",
    "                 va='center', \n",
    "                 xytext=(5, 0), \n",
    "                 textcoords='offset points')\n",
    "\n",
    "plt.title('Mean Treatment PD by Metastatic First Treatment Type')\n",
    "plt.xlabel('Mean Treatment PD')\n",
    "plt.ylabel('Metastatic First Treatment Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a count plot with switched axes and arranged regions alphabetically\n",
    "plt.figure(figsize=(15, 5))\n",
    "ax = sns.countplot(y='region', data=df, palette='autumn', order=sorted(df['region'].unique()))\n",
    "\n",
    "# Annotate each bar with its count value\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_width(), '.0f'), \n",
    "                 (p.get_width(), p.get_y() + p.get_height() / 2), \n",
    "                 ha='left', \n",
    "                 va='center', \n",
    "                 xytext=(5, 0), \n",
    "                 textcoords='offset points')\n",
    "\n",
    "plt.title('Count of Regions')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Region')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean treatment_pd for each region\n",
    "mean_treatment_pd_region = df.groupby('region')['treatment_pd'].mean().reset_index()\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "ax = sns.barplot(x='treatment_pd', y='region', data=mean_treatment_pd_region, palette='autumn', orient='h')\n",
    "\n",
    "# Annotate each bar with its mean treatment PD value\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_width(), '.2f'), \n",
    "                 (p.get_width(), p.get_y() + p.get_height() / 2), \n",
    "                 ha='left', \n",
    "                 va='center', \n",
    "                 xytext=(5, 0), \n",
    "                 textcoords='offset points')\n",
    "\n",
    "plt.title('Mean Treatment PD by Region')\n",
    "plt.xlabel('Mean Treatment PD')\n",
    "plt.ylabel('Region')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a count plot with switched axes and arranged divisions alphabetically\n",
    "plt.figure(figsize=(15, 5))\n",
    "ax = sns.countplot(y='division', data=df, palette='autumn', order=sorted(df['division'].unique()))\n",
    "\n",
    "# Annotate each bar with its count value\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_width(), '.0f'), \n",
    "                 (p.get_width(), p.get_y() + p.get_height() / 2), \n",
    "                 ha='left', \n",
    "                 va='center', \n",
    "                 xytext=(5, 0), \n",
    "                 textcoords='offset points')\n",
    "\n",
    "plt.title('Count of Divisions')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Division')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean treatment_pd for each division\n",
    "mean_treatment_pd_division = df.groupby('division')['treatment_pd'].mean().reset_index()\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "ax = sns.barplot(x='treatment_pd', y='division', data=mean_treatment_pd_division, palette='autumn', orient='h')\n",
    "\n",
    "# Annotate each bar with its mean treatment PD value\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_width(), '.2f'), \n",
    "                 (p.get_width(), p.get_y() + p.get_height() / 2), \n",
    "                 ha='left', \n",
    "                 va='center', \n",
    "                 xytext=(5, 0), \n",
    "                 textcoords='offset points')\n",
    "\n",
    "plt.title('Mean Treatment PD by Division')\n",
    "plt.xlabel('Mean Treatment PD')\n",
    "plt.ylabel('Division')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning for 'rent_burden'\n",
    "bins = [-1, 25, 50, 75, 100]\n",
    "\n",
    "# Creating new DataFrame to combine 'Treatment_PD' with 'rent_burden'\n",
    "df_combined = pd.DataFrame({\n",
    "    'rent_burden_bins': pd.cut(df['rent_burden'], bins=bins),\n",
    "    'Treatment_PD': df['treatment_pd']\n",
    "})\n",
    "\n",
    "# Plotting the stacked bar plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "ax = sns.barplot(x='rent_burden_bins', y='Treatment_PD', data=df_combined, palette=\"autumn\", ci=None)\n",
    "plt.title('Relationship between Rent burden and Duration between Diagnosis and Treatment')\n",
    "plt.xlabel('Rent Burden (%)')\n",
    "plt.ylabel('Duration between Diagnosis and Treatment')\n",
    "\n",
    "# Annotating each bar with its value\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.2f'), \n",
    "                 (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                 ha='center', \n",
    "                 va='center', \n",
    "                 xytext=(0, 5), \n",
    "                 textcoords='offset points',\n",
    "                 fontsize=10)\n",
    "\n",
    "# Changing x-axis labels\n",
    "new_labels = ['0-25', '26-50', '51-75', '76-100']\n",
    "ax.set_xticklabels(new_labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning for 'race_white'\n",
    "bins = [-1, 25, 50, 75, 100]\n",
    "\n",
    "# Creating new DataFrame to combine 'Treatment_PD' with 'race_white'\n",
    "df_combined = pd.DataFrame({\n",
    "    'race_white_bins': pd.cut(df['race_white'], bins=bins),\n",
    "    'Treatment_PD': df['treatment_pd']\n",
    "})\n",
    "\n",
    "# Plotting the stacked bar plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "ax = sns.barplot(x='race_white_bins', y='Treatment_PD', data=df_combined, palette=\"autumn\", ci=None)\n",
    "plt.title('Relationship between White Race population and Duration between Diagnosis and Treatment')\n",
    "plt.xlabel('White Race (%)')\n",
    "plt.ylabel('Duration between Diagnosis and Treatment')\n",
    "\n",
    "# Annotating each bar with its value\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.2f'), \n",
    "                 (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                 ha='center', \n",
    "                 va='center', \n",
    "                 xytext=(0, 5), \n",
    "                 textcoords='offset points',\n",
    "                 fontsize=10)\n",
    "\n",
    "# Changing x-axis labels\n",
    "new_labels = ['0-25', '26-50', '51-75', '76-100']\n",
    "ax.set_xticklabels(new_labels)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning for 'race_other'\n",
    "bins = [-1, 10, 20, 30, 40]\n",
    "\n",
    "# Creating new DataFrame to combine 'Treatment_PD' with 'race_other'\n",
    "df_combined = pd.DataFrame({\n",
    "    'race_other_bins': pd.cut(df['race_other'], bins=bins),\n",
    "    'Treatment_PD': df['treatment_pd']\n",
    "})\n",
    "\n",
    "# Plotting the stacked bar plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "ax = sns.barplot(x='race_other_bins', y='Treatment_PD', data=df_combined, palette=\"autumn\", ci=None)\n",
    "plt.title('Relationship between Other Race population and Duration between Diagnosis and Treatment')\n",
    "plt.xlabel('Other Race (%)')\n",
    "plt.ylabel('Duration between Diagnosis and Treatment')\n",
    "\n",
    "# Annotating each bar with its value\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.2f'), \n",
    "                 (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                 ha='center', \n",
    "                 va='center', \n",
    "                 xytext=(0, 5), \n",
    "                 textcoords='offset points',\n",
    "                 fontsize=10)\n",
    "\n",
    "# Changing x-axis labels\n",
    "new_labels = ['0-10', '11-20', '21-30', '31-40']\n",
    "ax.set_xticklabels(new_labels)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning for 'unemployment_rate'\n",
    "bins = [-1, 5, 10, 15, 20]\n",
    "\n",
    "# Creating new DataFrame to combine 'Treatment_PD' with 'unemployment_rate'\n",
    "df_combined = pd.DataFrame({\n",
    "    'unemployment_rate_bins': pd.cut(df['unemployment_rate'], bins=bins),\n",
    "    'Treatment_PD': df['treatment_pd']\n",
    "})\n",
    "\n",
    "# Plotting the stacked bar plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "ax = sns.barplot(x='unemployment_rate_bins', y='Treatment_PD', data=df_combined, palette=\"autumn\", ci=None)\n",
    "plt.title('Relationship between Unemployment Rate and Duration between Diagnosis and Treatment')\n",
    "plt.xlabel('Unemployment Rate (%)')\n",
    "plt.ylabel('Duration between Diagnosis and Treatment')\n",
    "\n",
    "# Annotating each bar with its value\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.2f'), \n",
    "                 (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                 ha='center', \n",
    "                 va='center', \n",
    "                 xytext=(0, 5), \n",
    "                 textcoords='offset points',\n",
    "                 fontsize=10)\n",
    "\n",
    "# Changing x-axis labels\n",
    "new_labels = ['0-5', '6-10', '11-15', '16-20']\n",
    "ax.set_xticklabels(new_labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning for 'limited_english'\n",
    "bins = [-1, 20, 40, 60, 80]\n",
    "\n",
    "# Creating new DataFrame to combine 'Treatment_PD' with 'limited_english'\n",
    "df_combined = pd.DataFrame({\n",
    "    'limited_english_bins': pd.cut(df['limited_english'], bins=bins),\n",
    "    'Treatment_PD': df['treatment_pd']\n",
    "})\n",
    "\n",
    "# Plotting the stacked bar plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "ax = sns.barplot(x='limited_english_bins', y='Treatment_PD', data=df_combined, palette=\"autumn\", ci=None)\n",
    "plt.title('Relationship between Limited English Proficiency and Duration between Diagnosis and Treatment')\n",
    "plt.xlabel('Limited English Proficiency (%)')\n",
    "plt.ylabel('Duration between Diagnosis and Treatment')\n",
    "\n",
    "# Annotating each bar with its value\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.2f'), \n",
    "                 (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                 ha='center', \n",
    "                 va='center', \n",
    "                 xytext=(0, 5), \n",
    "                 textcoords='offset points',\n",
    "                 fontsize=10)\n",
    "\n",
    "# Changing x-axis labels\n",
    "new_labels = ['0-20', '21-40', '41-60', '61-80']\n",
    "ax.set_xticklabels(new_labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning for 'home_value'\n",
    "bins = [-1, 500000, 1000000, 1500000, 2000000]\n",
    "\n",
    "# Creating new DataFrame to combine 'Treatment_PD' with 'home_value'\n",
    "df_combined = pd.DataFrame({\n",
    "    'home_value_bins': pd.cut(df['home_value'], bins=bins),\n",
    "    'Treatment_PD': df['treatment_pd']\n",
    "})\n",
    "\n",
    "# Plotting the stacked bar plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "ax = sns.barplot(x='home_value_bins', y='Treatment_PD', data=df_combined, palette=\"autumn\", ci=None)\n",
    "plt.title('Relationship between Median Home Value and Duration between Diagnosis and Treatment')\n",
    "plt.xlabel('Median Home Value ($)')\n",
    "plt.ylabel('Duration between Diagnosis and Treatment')\n",
    "\n",
    "# Annotating each bar with its value\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.2f'), \n",
    "                 (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                 ha='center', \n",
    "                 va='center', \n",
    "                 xytext=(0, 5), \n",
    "                 textcoords='offset points',\n",
    "                 fontsize=10)\n",
    "\n",
    "# Changing x-axis labels\n",
    "new_labels = ['0-500000', '500001-1000000', '1000001-1500000', '1500001-2000000']\n",
    "ax.set_xticklabels(new_labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning for 'home_ownership'\n",
    "bins = [-1, 25, 50, 75, 100]\n",
    "\n",
    "# Creating a new DataFrame to combine 'Treatment_PD' with 'home_ownership'\n",
    "df_combined = pd.DataFrame({\n",
    "    'home_ownership_bins': pd.cut(df['home_ownership'], bins=bins),\n",
    "    'Treatment_PD': df['treatment_pd']\n",
    "})\n",
    "\n",
    "# Plotting the stacked bar plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "ax = sns.barplot(x='home_ownership_bins', y='Treatment_PD', data=df_combined, palette=\"autumn\", ci=None)\n",
    "plt.title('Relationship between Home Ownership and Duration between Diagnosis and Treatment')\n",
    "plt.xlabel('Home Ownership (%)')\n",
    "plt.ylabel('Duration between Diagnosis and Treatment')\n",
    "\n",
    "# Annotating each bar with its value\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.2f'), \n",
    "                 (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                 ha='center', \n",
    "                 va='center', \n",
    "                 xytext=(0, 5), \n",
    "                 textcoords='offset points',\n",
    "                 fontsize=10)\n",
    "\n",
    "# Changing x-axis labels\n",
    "new_labels = ['0-25', '26-50', '51-75', '76-100']\n",
    "ax.set_xticklabels(new_labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Mean_treatment_pd', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Median_treatment_pd', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('treatment_pd_bins', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target variable (y)\n",
    "X = df.drop(columns=['treatment_pd'])\n",
    "y = df['treatment_pd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure proper encoding of categorical variables\n",
    "X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for remaining string columns\n",
    "remaining_str_cols = X.select_dtypes(include=['object']).columns\n",
    "if not remaining_str_cols.empty:\n",
    "    raise ValueError(f\"Remaining string columns: {remaining_str_cols}. Please address these before proceeding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 1 - Local Outlier Factor Anomaly Detection, VarianceThreshold Feature Selection, and Decision Tree Regression Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# Create the Local Outlier Factor anomaly detection model\n",
    "lof = LocalOutlierFactor(contamination=0.1)\n",
    "\n",
    "# Fit the Local Outlier Factor model on the training data\n",
    "lof.fit(X_train)\n",
    "\n",
    "# Predict outliers in the test data\n",
    "outliers = lof.fit_predict(X_test)\n",
    "\n",
    "# Identify indices of inliers (non-outliers)\n",
    "inliers_indices = np.where(outliers == 1)\n",
    "\n",
    "# Select only inliers for training and testing data\n",
    "X_train_inliers = X_train.iloc[inliers_indices]\n",
    "y_train_inliers = y_train.iloc[inliers_indices]\n",
    "X_test_inliers = X_test.iloc[inliers_indices]\n",
    "y_test_inliers = y_test.iloc[inliers_indices]\n",
    "\n",
    "# Create the Decision Tree Regressor pipeline with VarianceThreshold for feature selection\n",
    "dt_pipeline_with_feature_selection = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_selection', VarianceThreshold()),\n",
    "    ('dt_model', DecisionTreeRegressor(random_state=55))\n",
    "])\n",
    "\n",
    "# Parameters grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'feature_selection__threshold': [0.1, 0.2, 0.3, 0.4, 0.5],  # Variance thresholds\n",
    "    'dt_model__min_samples_split': [0.1, 0.2, 0.3 , 0.4, 0.5],  # Minimum number of samples required to split an internal node\n",
    "    'dt_model__min_samples_leaf': [0.1, 0.2, 0.3, 0.4, 0.5]  # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Setup the GridSearchCV to find the best parameters\n",
    "grid_search = GridSearchCV(dt_pipeline_with_feature_selection, param_grid, cv=10, verbose=1, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Train the model with GridSearchCV to find the best parameters\n",
    "grid_search.fit(X_train_inliers, y_train_inliers)\n",
    "\n",
    "# Use the best estimator to predict on test inliers\n",
    "best_model = grid_search.best_estimator_\n",
    "dt_y_pred_with_feature_selection = best_model.predict(X_test_inliers)\n",
    "\n",
    "# Initialize lists to store evaluation metrics for 10 folds of the best model\n",
    "r_squared_list = []\n",
    "mae_list = []\n",
    "mse_list = []\n",
    "rmse_list = []\n",
    "\n",
    "# Perform 10-fold cross-validation for the best model\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=55)\n",
    "fold = 0\n",
    "for train_index, test_index in kf.split(X_train_inliers):\n",
    "    fold += 1\n",
    "    X_train_fold, X_val_fold = X_train_inliers.iloc[train_index], X_train_inliers.iloc[test_index]\n",
    "    y_train_fold, y_val_fold = y_train_inliers.iloc[train_index], y_train_inliers.iloc[test_index]\n",
    "    \n",
    "    best_model.fit(X_train_fold, y_train_fold)\n",
    "    y_pred_fold = best_model.predict(X_val_fold)\n",
    "    \n",
    "    # Calculate evaluation metrics for each fold\n",
    "    r_squared = r2_score(y_val_fold, y_pred_fold)\n",
    "    mae = mean_absolute_error(y_val_fold, y_pred_fold)\n",
    "    mse = mean_squared_error(y_val_fold, y_pred_fold)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Append to lists\n",
    "    r_squared_list.append(r_squared)\n",
    "    mae_list.append(mae)\n",
    "    mse_list.append(mse)\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "# Create a DataFrame to store evaluation metrics for each fold\n",
    "evaluation_dt = pd.DataFrame({\n",
    "    'Fold': range(1, 11),\n",
    "    'R-squared': r_squared_list,\n",
    "    'MAE': mae_list,\n",
    "    'MSE': mse_list,\n",
    "    'RMSE': rmse_list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best parameters found by GridSearchCV\n",
    "print(\"Best Parameters for the model:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print anomalies detected by Local Outlier Factor\n",
    "anomalies_detected_lof = X_test.index[~X_test.index.isin(X_test_inliers.index)]\n",
    "print(f'Anomalies detected by Local Outlier Factor: {anomalies_detected_lof}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print selected features by VarianceThreshold\n",
    "selected_features_dt = X_train_inliers.columns[best_model.named_steps['feature_selection'].get_support()]\n",
    "print(f'Selected features by VarianceThreshold: {selected_features_dt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metrics for the best model\n",
    "dt_r2_with_feature_selection = r2_score(y_test_inliers, dt_y_pred_with_feature_selection)\n",
    "dt_mae_with_feature_selection = mean_absolute_error(y_test_inliers, dt_y_pred_with_feature_selection)\n",
    "dt_mse_with_feature_selection = mean_squared_error(y_test_inliers, dt_y_pred_with_feature_selection)\n",
    "dt_rmse_with_feature_selection = np.sqrt(dt_mse_with_feature_selection)\n",
    "\n",
    "print(f'R-squared Score of the pipeline: {dt_r2_with_feature_selection}')\n",
    "print(f'Mean Absolute Error of the pipeline: {dt_mae_with_feature_selection}')\n",
    "print(f'Mean Squared Error of the pipeline: {dt_mse_with_feature_selection}')\n",
    "print(f'Root Mean Squared Error of the pipeline: {dt_rmse_with_feature_selection}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics for the cross validation of best model\n",
    "print(\"Evaluation Metrics for 10 fold cross validation of the best model:\")\n",
    "print(evaluation_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot of actual vs. predicted values\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(y_test_inliers, dt_y_pred_with_feature_selection, alpha=0.5, color='blue', label='Actual vs. Predicted')\n",
    "plt.plot([y_test_inliers.min(), y_test_inliers.max()], [y_test_inliers.min(), y_test_inliers.max()], 'k--', lw=4, label='Ideal Line')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs. Predicted Values for the Decision Tree Pipeline)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals = y_test_inliers - dt_y_pred_with_feature_selection\n",
    "\n",
    "# Create a histogram of residuals\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.hist(residuals, bins=20, color='blue', alpha=0.5, label='Residuals')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Residuals of the Decision Tree Regressor Pipeline')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Pipeline 2 - Elliptic Envelope Anomaly Detection, Principal Component Analysis Feature Selection, and Random Forest Regression Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Create the Elliptic Envelope anomaly detection model\n",
    "elliptic_env = EllipticEnvelope(contamination=0.1, random_state=55)\n",
    "\n",
    "# Fit the model on the training data\n",
    "elliptic_env.fit(X_train)\n",
    "\n",
    "# Predict outliers in the test data\n",
    "outliers = elliptic_env.predict(X_test)\n",
    "\n",
    "# Identify indices of inliers (non-outliers)\n",
    "inliers_indices = np.where(outliers == 1)\n",
    "\n",
    "# Select only inliers for training and testing data\n",
    "X_train_inliers = X_train.iloc[inliers_indices]\n",
    "y_train_inliers = y_train.iloc[inliers_indices]\n",
    "X_test_inliers = X_test.iloc[inliers_indices]\n",
    "y_test_inliers = y_test.iloc[inliers_indices]\n",
    "\n",
    "# Create the Random Forest Regressor pipeline with PCA for feature selection\n",
    "rf_pipeline_with_feature_selection = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_selection', PCA()),\n",
    "    ('rf_model', RandomForestRegressor(random_state=55))\n",
    "])\n",
    "\n",
    "# Parameters grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'feature_selection__n_components': [0.9, 0.8, 0.7, 0.6, 0.5],  # Percentage of variance explained by the selected components\n",
    "    'rf_model__min_samples_split': [0.1, 0.2, 0.3, 0.4, 0.5],  # Minimum number of samples required to split an internal node\n",
    "    'rf_model__min_samples_leaf': [0.1, 0.2, 0.3, 0.4, 0.5]  # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Setup the GridSearchCV to find the best parameters\n",
    "grid_search = GridSearchCV(rf_pipeline_with_feature_selection, param_grid, cv=10, verbose=1, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Train the model with GridSearchCV to find the best parameters\n",
    "grid_search.fit(X_train_inliers, y_train_inliers)\n",
    "\n",
    "# Use the best estimator to predict on test inliers\n",
    "best_model = grid_search.best_estimator_\n",
    "rf_y_pred_with_feature_selection = best_model.predict(X_test_inliers)\n",
    "\n",
    "# Initialize lists to store evaluation metrics for 10 folds of the best model\n",
    "r_squared_list = []\n",
    "mae_list = []\n",
    "mse_list = []\n",
    "rmse_list = []\n",
    "\n",
    "# Perform 10-fold cross-validation for the best model\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=55)\n",
    "fold = 0\n",
    "for train_index, test_index in kf.split(X_train_inliers):\n",
    "    fold += 1\n",
    "    X_train_fold, X_val_fold = X_train_inliers.iloc[train_index], X_train_inliers.iloc[test_index]\n",
    "    y_train_fold, y_val_fold = y_train_inliers.iloc[train_index], y_train_inliers.iloc[test_index]\n",
    "    \n",
    "    best_model.fit(X_train_fold, y_train_fold)\n",
    "    y_pred_fold = best_model.predict(X_val_fold)\n",
    "    \n",
    "    # Calculate evaluation metrics for each fold\n",
    "    r_squared = r2_score(y_val_fold, y_pred_fold)\n",
    "    mae = mean_absolute_error(y_val_fold, y_pred_fold)\n",
    "    mse = mean_squared_error(y_val_fold, y_pred_fold)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Append to lists\n",
    "    r_squared_list.append(r_squared)\n",
    "    mae_list.append(mae)\n",
    "    mse_list.append(mse)\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "# Create a DataFrame to store evaluation metrics for each fold\n",
    "evaluation_rf = pd.DataFrame({\n",
    "    'Fold': range(1, 11),\n",
    "    'R-squared': r_squared_list,\n",
    "    'MAE': mae_list,\n",
    "    'MSE': mse_list,\n",
    "    'RMSE': rmse_list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best parameters found by GridSearchCV\n",
    "print(\"Best Parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print anomalies detected by Elliptic Envelope\n",
    "anomalies_detected_elliptic = X_test.index[~X_test.index.isin(X_test_inliers.index)]\n",
    "print(f'Anomalies detected by Elliptic Envelope: {anomalies_detected_elliptic}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print selected features by PCA\n",
    "selected_features_pca = [f'PC{i+1}' for i in range(best_model.named_steps['feature_selection'].n_components_)]\n",
    "print(f'Selected features by PCA: {selected_features_pca}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics\n",
    "rf_r2_with_feature_selection = r2_score(y_test_inliers, rf_y_pred_with_feature_selection)\n",
    "rf_mae_with_feature_selection = mean_absolute_error(y_test_inliers, rf_y_pred_with_feature_selection)\n",
    "rf_mse_with_feature_selection = mean_squared_error(y_test_inliers, rf_y_pred_with_feature_selection)\n",
    "rf_rmse_with_feature_selection = np.sqrt(rf_mse_with_feature_selection)\n",
    "\n",
    "print(f'R-squared Score of the pipeline: {rf_r2_with_feature_selection}')\n",
    "print(f'Mean Absolute Error of the pipeline: {rf_mae_with_feature_selection}')\n",
    "print(f'Mean Squared Error of the pipeline: {rf_mse_with_feature_selection}')\n",
    "print(f'Root Mean Squared Error of the pipeline: {rf_rmse_with_feature_selection}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics for the cross validation of best model\n",
    "print(\"Evaluation Metrics for 10 fold cross validation of the best model:\")\n",
    "print(evaluation_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot of actual vs. predicted values\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(y_test_inliers, rf_y_pred_with_feature_selection, alpha=0.5, color='blue', label='Actual vs. Predicted')\n",
    "plt.plot([y_test_inliers.min(), y_test_inliers.max()], [y_test_inliers.min(), y_test_inliers.max()], 'k--', lw=4, label='Ideal Line')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs. Predicted Values for the Random Forest Regressor Pipeline)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals = y_test_inliers - rf_y_pred_with_feature_selection\n",
    "\n",
    "# Create a histogram of residuals\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.hist(residuals, bins=20, color='blue', alpha=0.5, label='Residuals')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Residuals of the Decision Tree Regressor Pipeline')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 3 - Isolation Forest Anomaly Detection, Recursive Feature Elimination Feature Selection, and Gradient Boosting Regression Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Create the Isolation Forest anomaly detection model\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=55)\n",
    "\n",
    "# Fit the Isolation Forest model on the training data\n",
    "iso_forest.fit(X_train)\n",
    "\n",
    "# Predict outliers in the test data\n",
    "outliers = iso_forest.predict(X_test)\n",
    "\n",
    "# Identify indices of inliers (non-outliers)\n",
    "inliers_indices = np.where(outliers == 1)\n",
    "\n",
    "# Select only inliers for training and testing data\n",
    "X_train_inliers = X_train.iloc[inliers_indices]\n",
    "y_train_inliers = y_train.iloc[inliers_indices]\n",
    "X_test_inliers = X_test.iloc[inliers_indices]\n",
    "y_test_inliers = y_test.iloc[inliers_indices]\n",
    "\n",
    "# Create the Gradient Boosting Regressor pipeline with RFE for feature selection\n",
    "gb_pipeline_with_feature_selection = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_selection', RFE(estimator=GradientBoostingRegressor(random_state=55))),\n",
    "    ('gb_model', GradientBoostingRegressor(random_state=55))\n",
    "])\n",
    "\n",
    "# Parameters grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'feature_selection__n_features_to_select': [110, 120, 130, 140, 150],  # Number of features to select\n",
    "    'gb_model__learning_rate': [0.1, 0.2 0.3, 0.4, 0.5],  # Shrinks the contribution of each tree\n",
    "    'gb_model__max_depth': [1, 2, 3, 4, 5]  # Maximum depth of the individual regression estimators\n",
    "}\n",
    "\n",
    "# Setup the GridSearchCV to find the best parameters\n",
    "grid_search = GridSearchCV(gb_pipeline_with_feature_selection, param_grid, cv=10, verbose=1, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Train the model with GridSearchCV to find the best parameters\n",
    "grid_search.fit(X_train_inliers, y_train_inliers)\n",
    "\n",
    "# Use the best estimator to predict on test inliers\n",
    "best_model = grid_search.best_estimator_\n",
    "gb_y_pred_with_feature_selection = best_model.predict(X_test_inliers)\n",
    "\n",
    "# Initialize lists to store evaluation metrics for 10 folds of the best model\n",
    "r_squared_list = []\n",
    "mae_list = []\n",
    "mse_list = []\n",
    "rmse_list = []\n",
    "\n",
    "# Perform 10-fold cross-validation for the best model\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "fold = 0\n",
    "for train_index, test_index in kf.split(X_train_inliers):\n",
    "    fold += 1\n",
    "    X_train_fold, X_val_fold = X_train_inliers.iloc[train_index], X_train_inliers.iloc[test_index]\n",
    "    y_train_fold, y_val_fold = y_train_inliers.iloc[train_index], y_train_inliers.iloc[test_index]\n",
    "    \n",
    "    best_model.fit(X_train_fold, y_train_fold)\n",
    "    y_pred_fold = best_model.predict(X_val_fold)\n",
    "    \n",
    "    # Calculate evaluation metrics for each fold\n",
    "    r_squared = r2_score(y_val_fold, y_pred_fold)\n",
    "    mae = mean_absolute_error(y_val_fold, y_pred_fold)\n",
    "    mse = mean_squared_error(y_val_fold, y_pred_fold)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Append to lists\n",
    "    r_squared_list.append(r_squared)\n",
    "    mae_list.append(mae)\n",
    "    mse_list.append(mse)\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "# Create a DataFrame to store evaluation metrics for each fold\n",
    "evaluation_gb = pd.DataFrame({\n",
    "    'Fold': range(1, 11),\n",
    "    'R-squared': r_squared_list,\n",
    "    'MAE': mae_list,\n",
    "    'MSE': mse_list,\n",
    "    'RMSE': rmse_list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best parameters found by GridSearchCV\n",
    "print(\"Best Parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print anomalies detected by Isolation Forest\n",
    "anomalies_detected_iso = X_test.index[~X_test.index.isin(X_test_inliers.index)]\n",
    "print(f'Anomalies detected by Isolation Forest: {anomalies_detected_iso}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print selected features by RFE\n",
    "selected_features_gb = X_train_inliers.columns[best_model.named_steps['feature_selection'].support_]\n",
    "print(f'Selected features by RFE: {selected_features_gb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics\n",
    "gb_r2_with_feature_selection = r2_score(y_test_inliers, gb_y_pred_with_feature_selection)\n",
    "gb_mae_with_feature_selection = mean_absolute_error(y_test_inliers, gb_y_pred_with_feature_selection)\n",
    "gb_mse_with_feature_selection = mean_squared_error(y_test_inliers, gb_y_pred_with_feature_selection)\n",
    "gb_rmse_with_feature_selection = np.sqrt(gb_mse_with_feature_selection)\n",
    "\n",
    "\n",
    "print(f'R-squared Score of the pipeline: {gb_r2_with_feature_selection}')\n",
    "print(f'Mean Absolute Error of the pipeline: {gb_mae_with_feature_selection}')\n",
    "print(f'Mean Squared Error of the pipeline: {gb_mse_with_feature_selection}')\n",
    "print(f'Root Mean Squared Error of the pipeline: {gb_rmse_with_feature_selection}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics for the cross validation of best model\n",
    "print(\"Evaluation Metrics for 10 fold cross validation of the best model:\")\n",
    "print(evaluation_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot of actual vs. predicted values\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(y_test_inliers, gb_y_pred_with_feature_selection, alpha=0.5, color='blue', label='Actual vs. Predicted')\n",
    "plt.plot([y_test_inliers.min(), y_test_inliers.max()], [y_test_inliers.min(), y_test_inliers.max()], 'k--', lw=4, label='Ideal Line')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs. Predicted Values for the Gradient Boosting Regressor Pipeline')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals = y_test_inliers - gb_y_pred_with_feature_selection\n",
    "\n",
    "# Create a histogram of residuals\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.hist(residuals, bins=20, color='blue', alpha=0.5, label='Residuals')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Residuals of the Gradient Boosting Regressor Pipeline')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 4 - One-Class SVM Anomaly Detection, SelectFromModel Feature Selection, and Extreme Gradient Boosting Regression Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Create the One-Class SVM anomaly detection model with adjusted parameters\n",
    "one_class_svm = OneClassSVM(nu=0.1, kernel='rbf', gamma='scale')\n",
    "\n",
    "# Fit the model on the training data\n",
    "one_class_svm.fit(X_train)\n",
    "\n",
    "# Predict outliers in the test data\n",
    "outliers = one_class_svm.predict(X_test)\n",
    "\n",
    "# Identify indices of inliers (non-outliers)\n",
    "inliers_indices = np.where(outliers == 1)\n",
    "\n",
    "# Select only inliers for training and testing data\n",
    "X_train_inliers = X_train.iloc[inliers_indices]\n",
    "y_train_inliers = y_train.iloc[inliers_indices]\n",
    "X_test_inliers = X_test.iloc[inliers_indices]\n",
    "y_test_inliers = y_test.iloc[inliers_indices]\n",
    "\n",
    "# Create the XGBoost Regressor pipeline with SelectFromModel for feature selection\n",
    "xgb_pipeline_with_feature_selection = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_selection', SelectFromModel(estimator=XGBRegressor(random_state=55))),\n",
    "    ('xgb_model', XGBRegressor(random_state=55))\n",
    "])\n",
    "\n",
    "# Parameters grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'feature_selection__threshold': [0.01, 0.02, 0.03, 0.04, 0.05],  # Threshold for features selection\n",
    "    'xgb_model__learning_rate': [0.1, 0.2, 0.3, 0.4, 0.5],  # Learning rate\n",
    "    'xgb_model__max_depth': [1, 2, 3, 4, 5]  # Maximum depth of the trees\n",
    "}\n",
    "\n",
    "# Setup the GridSearchCV to find the best parameters\n",
    "grid_search = GridSearchCV(xgb_pipeline_with_feature_selection, param_grid, cv=10, verbose=1, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Train the model with GridSearchCV to find the best parameters\n",
    "grid_search.fit(X_train_inliers, y_train_inliers)\n",
    "\n",
    "# Use the best estimator to predict on test inliers\n",
    "best_model = grid_search.best_estimator_\n",
    "xgb_y_pred_with_feature_selection = best_model.predict(X_test_inliers)\n",
    "\n",
    "# Initialize lists to store evaluation metrics for 10 folds of the best model\n",
    "r_squared_list = []\n",
    "mae_list = []\n",
    "mse_list = []\n",
    "rmse_list = []\n",
    "\n",
    "# Perform 10-fold cross-validation for the best model\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=55)\n",
    "fold = 0\n",
    "for train_index, test_index in kf.split(X_train_inliers):\n",
    "    fold += 1\n",
    "    X_train_fold, X_val_fold = X_train_inliers.iloc[train_index], X_train_inliers.iloc[test_index]\n",
    "    y_train_fold, y_val_fold = y_train_inliers.iloc[train_index], y_train_inliers.iloc[test_index]\n",
    "    \n",
    "    best_model.fit(X_train_fold, y_train_fold)\n",
    "    y_pred_fold = best_model.predict(X_val_fold)\n",
    "    \n",
    "    # Calculate evaluation metrics for each fold\n",
    "    r_squared = r2_score(y_val_fold, y_pred_fold)\n",
    "    mae = mean_absolute_error(y_val_fold, y_pred_fold)\n",
    "    mse = mean_squared_error(y_val_fold, y_pred_fold)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Append to lists\n",
    "    r_squared_list.append(r_squared)\n",
    "    mae_list.append(mae)\n",
    "    mse_list.append(mse)\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "# Create a DataFrame to store evaluation metrics for each fold\n",
    "evaluation_xgb = pd.DataFrame({\n",
    "    'Fold': range(1, 11),\n",
    "    'R-squared': r_squared_list,\n",
    "    'MAE': mae_list,\n",
    "    'MSE': mse_list,\n",
    "    'RMSE': rmse_list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the best parameters\n",
    "print(\"Best parameters found:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print anomalies detected by One-Class SVM\n",
    "anomalies_detected_svm = X_test.index[~X_test.index.isin(X_test_inliers.index)]\n",
    "print(f'Anomalies detected by One-Class SVM: {anomalies_detected_svm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print selected features by SelectFromModel\n",
    "selected_features_xgb = X_train_inliers.columns[best_model.named_steps['feature_selection'].get_support()]\n",
    "print(f'Selected features by SelectFromModel: {selected_features_xgb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics\n",
    "xgb_r2_with_feature_selection = r2_score(y_test_inliers, xgb_y_pred_with_feature_selection)\n",
    "xgb_mae_with_feature_selection = mean_absolute_error(y_test_inliers, xgb_y_pred_with_feature_selection)\n",
    "xgb_mse_with_feature_selection = mean_squared_error(y_test_inliers, xgb_y_pred_with_feature_selection)\n",
    "xgb_rmse_with_feature_selection = np.sqrt(xgb_mse_with_feature_selection)\n",
    "\n",
    "\n",
    "print(f'R-squared Score of the pipeline: {xgb_r2_with_feature_selection}')\n",
    "print(f'Mean Absolute Error of the pipeline: {xgb_mae_with_feature_selection}')\n",
    "print(f'Mean Squared Error of the pipeline: {xgb_mse_with_feature_selection}')\n",
    "print(f'Root Mean Squared Error of the pipeline: {xgb_rmse_with_feature_selection}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics for the cross validation of best model\n",
    "print(\"Evaluation Metrics for 10 fold cross validation of the best model:\")\n",
    "print(evaluation_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot of actual vs. predicted values\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(y_test_inliers, xgb_y_pred_with_feature_selection, alpha=0.5, color='blue', label='Actual vs. Predicted')\n",
    "plt.plot([y_test_inliers.min(), y_test_inliers.max()], [y_test_inliers.min(), y_test_inliers.max()], 'k--', lw=4, label='Ideal Line')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs. Predicted Values for the Extreme Gradient Boost Regressor Pipeline')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals = y_test_inliers - xgb_y_pred_with_feature_selection\n",
    "\n",
    "# Create a histogram of residuals\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.hist(residuals, bins=20, color='blue', alpha=0.5, label='Residuals')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Residuals of the Extreme Gradient Boost Regressor Pipeline')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
